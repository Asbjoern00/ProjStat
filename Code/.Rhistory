w7 <- rnorm(n = n)
w8 <- rbinom(n,1,0.65)*w7
w9 <- rnorm(n = n)
w10 <- rbinom(n,1,0.35)*w9
W <- cbind(w1,w2,w3,w4,w5,w6,w7,w8,w9,w10)
W
}
sim_cov(10)
sim_cov(10)
a <- sim_cov(10)
a
a[1]
a[1,1]
a[,1]
a[1,]
sim(W)
#Make a function that generates ten covariates and applies some transformation of these
sim_cov <- function(n = 100){
w1 <- rnorm(1, n = n)
w2 <- rbinom(n,1,0.65)*w1
w3 <- rnorm(-1,n = n)
w4 <- rbinom(n,1,0.65)*w2
w5 <- rnorm(n = n)
w6 <- rbinom(n,1,0.47)*w1
w7 <- rnorm(n = n)
w8 <- rbinom(n,1,0.65)*w7
w9 <- rnorm(n = n)
w10 <- rbinom(n,1,0.35)*w9
W <- cbind(w1,w2,w3,w4,w5,w6,w7,w8,w9,w10)
W
}
W <- sim_cov()
sim_A(W)
#Function that takes output from sim_cov and simulates A using some non-linear function of W
sim_A <- function(W){
prob_A <- logit(exp(W[,1]) + sin(W[,2] + 0.5*W[,3] + 0.4*cos(W[,4]) + 0.3*sin(W[,5]) + 0.2*cos(W[,6]) + 0.1*sin(W[,7]) + 0.1*cos(W[,8]) + 0.1*sin(W[,9]) + 0.1*cos(W[,10])))
A <- rbinom(n = nrow(W), size = 1, prob = prob_A)
A
}
sim(A)
sim_(A)
sim_A(W)
mean(sim_A)
mean(sim_A(W))
source("~/Desktop/ProjStat/Code/simulate.R")
Y
mean(Y)
A
W
source("~/Desktop/ProjStat/Code/simulate.R")
prob_Y
prob_Y <- logit(0.5 + / 0.5*exp(W[,1]) * 0.5*sin(W[,2]) + 0.5*W[,3] + 0.4*cos(W[,4]) + 0.3*sin(W[,5]) + 0.2*cos(W[,6]) + 0.1*sin(W[,7]) + 0.1*cos(W[,8]) + 0.1*sin(W[,9]) + 0.1*cos(W[,10]))
prob_Y <- logit(0.5 +  0.5*sin(W[,2]) + 0.5*W[,3] + 0.4*cos(W[,4]) + 0.3*sin(W[,5]) + 0.2*cos(W[,6]) + 0.1*sin(W[,7]) + 0.1*cos(W[,8]) + 0.1*sin(W[,9]) + 0.1*cos(W[,10]))
prob_Y
prob_Y2 <- logit(0.5 + 0.5 / 0.5*exp(W[,1]) * 0.5*sin(W[,2]) + 0.5*W[,3] + 0.4*cos(W[,4]) + 0.3*sin(W[,5]) + 0.2*cos(W[,6]) + 0.1*sin(W[,7]) + 0.1*cos(W[,8]) + 0.1*sin(W[,9]) + 0.1*cos(W[,10]))
prob_Y
prob_Y-probY2
prob_Y-prob_Y2
mean(prob_Y-prob_Y2)
pY(A) <- function(A,W){
logit(0.5 + 0.5*A / 0.5*exp(W[,1]) * 0.5*sin(W[,2]) + 0.5*W[,3] + 0.4*cos(W[,4]) + 0.3*sin(W[,5]) + 0.2*cos(W[,6]) + 0.1*sin(W[,7]) + 0.1*cos(W[,8]) + 0.1*sin(W[,9]) + 0.1*cos(W[,10]))
}
pY <- function(A,W){
logit(0.5 + 0.5*A / 0.5*exp(W[,1]) * 0.5*sin(W[,2]) + 0.5*W[,3] + 0.4*cos(W[,4]) + 0.3*sin(W[,5]) + 0.2*cos(W[,6]) + 0.1*sin(W[,7]) + 0.1*cos(W[,8]) + 0.1*sin(W[,9]) + 0.1*cos(W[,10]))
}
pY(A,"")
pY(A,W)
prob_Y <- logit(0.5 + 0.5*A / 0.5*exp(W[,1]) * 0.5*sin(W[,2]) + 0.5*W[,3] + 0.4*cos(W[,4]) + 0.3*sin(W[,5]) + 0.2*cos(W[,6]) + 0.1*sin(W[,7]) + 0.1*cos(W[,8]) + 0.1*sin(W[,9]) + 0.1*cos(W[,10]))
pY(A,W)
pY(A,W)-prob_Y
W
tibble(cbind(W,A,Y))
tibble(cbind(W,A))
tibble(W = W, A = A, Y = Y)
#Simulate Y
Y_lst <- sim_Y(A,W)
source("~/Desktop/ProjStat/Code/simulate.R")
out_frame
A
mean(A)
#Function that takes output from sim_cov and simulates A using some non-linear function of W
sim_A <- function(W){
prob_A <- logit(exp(W[,1]-1) + sin(W[,2] + 0.5*W[,3] + 0.4*cos(W[,4]) + 0.3*sin(W[,5]) + 0.2*cos(W[,6]) + 0.1*sin(W[,7]) + 0.1*cos(W[,8]) + 0.1*sin(W[,9]) + 0.1*cos(W[,10])))
A <- rbinom(n = nrow(W), size = 1, prob = prob_A)
A
}
source("~/Desktop/ProjStat/Code/simulate.R")
A
mean(A)
source("~/Desktop/ProjStat/Code/simulate.R")
A
mean(A)
ATE
source("~/Desktop/ProjStat/Code/simulate.R")
source("~/Desktop/ProjStat/Code/simulate.R")
source("~/Desktop/ProjStat/Code/simulate.R")
cbind(W, A, Y)
cbind(W, A, Y) %>% as_tibble()
#readRDS("/home/asr/Desktop/ProjStat/Data/processeddata.rds")
library(tidyverse)
#Make a function that generates ten covariates and applies some transformation of these
sim_cov <- function(n = 100){
w1 <- rnorm(1, n = n)
w2 <- rbinom(n,1,0.65)*w1
w3 <- rnorm(-1,n = n)
w4 <- rbinom(n,1,0.65)*w2
w5 <- rnorm(n = n)
w6 <- rbinom(n,1,0.47)*w1
w7 <- rnorm(n = n)
w8 <- rbinom(n,1,0.65)*w7
w9 <- rnorm(n = n)
w10 <- rbinom(n,1,0.35)*w9
W <- cbind(w1,w2,w3,w4,w5,w6,w7,w8,w9,w10)
W
}
logit <- function(x){
exp(x)/(1+exp(x))
}
#Function that takes output from sim_cov and simulates A using some non-linear function of W
sim_A <- function(W){
prob_A <- logit(-exp(W[,1]-1) + sin(W[,2] + 0.5*W[,3] + 0.4*cos(W[,4]) + 0.3*sin(W[,5]) + 0.2*cos(W[,6]) + 0.1*sin(W[,7]) + 0.1*cos(W[,8]) + 0.1*sin(W[,9]) + 0.1*cos(W[,10])))
A <- rbinom(n = nrow(W), size = 1, prob = prob_A)
A
}
sim_Y <- function(A,W){
pY <- function(A,W){
logit(0.5 + 0.5*A / 0.5*exp(W[,1]) *sin(W[,2])^2 + 0.5*W[,3] + 0.4*cos(W[,4]) + 0.3*sin(W[,5]) + 0.2*cos(W[,6]) + 0.1*sin(W[,7]) + 0.1*cos(W[,8]) + 0.1*sin(W[,9]) + 0.1*cos(W[,10]))
}
prob_Y <- pY(A,W)
Y <- rbinom(n = nrow(W), size = 1, prob = prob_Y)
list(Y = Y, pY1 = pY(1,W), pY0 = pY(0,W))
}
simulate_from_model <- function(n = 100){
#Simulate covariates
W <- sim_cov(n)
#simulate A
A <- sim_A(W)
#Simulate Y
Y_lst <- sim_Y(A,W)
Y <- Y_lst$Y
ATE <- mean(Y_lst$pY1 - Y_lst$pY0)
# Bind W, A, Y together to tibble, such that the columns in W are named w1, w2, ..., w10
out_frame <- cbind(W, A, Y) %>% as_tibble()
list(
ATE = ATE,
out_frame = out_frame,
)
}
simulate_from_model()
#readRDS("/home/asr/Desktop/ProjStat/Data/processeddata.rds")
library(tidyverse)
#Make a function that generates ten covariates and applies some transformation of these
sim_cov <- function(n = 100){
w1 <- rnorm(1, n = n)
w2 <- rbinom(n,1,0.65)*w1
w3 <- rnorm(-1,n = n)
w4 <- rbinom(n,1,0.65)*w2
w5 <- rnorm(n = n)
w6 <- rbinom(n,1,0.47)*w1
w7 <- rnorm(n = n)
w8 <- rbinom(n,1,0.65)*w7
w9 <- rnorm(n = n)
w10 <- rbinom(n,1,0.35)*w9
W <- cbind(w1,w2,w3,w4,w5,w6,w7,w8,w9,w10)
W
}
logit <- function(x){
exp(x)/(1+exp(x))
}
#Function that takes output from sim_cov and simulates A using some non-linear function of W
sim_A <- function(W){
prob_A <- logit(-exp(W[,1]-1) + sin(W[,2] + 0.5*W[,3] + 0.4*cos(W[,4]) + 0.3*sin(W[,5]) + 0.2*cos(W[,6]) + 0.1*sin(W[,7]) + 0.1*cos(W[,8]) + 0.1*sin(W[,9]) + 0.1*cos(W[,10])))
A <- rbinom(n = nrow(W), size = 1, prob = prob_A)
A
}
sim_Y <- function(A,W){
pY <- function(A,W){
logit(0.5 + 0.5*A / 0.5*exp(W[,1]) *sin(W[,2])^2 + 0.5*W[,3] + 0.4*cos(W[,4]) + 0.3*sin(W[,5]) + 0.2*cos(W[,6]) + 0.1*sin(W[,7]) + 0.1*cos(W[,8]) + 0.1*sin(W[,9]) + 0.1*cos(W[,10]))
}
prob_Y <- pY(A,W)
Y <- rbinom(n = nrow(W), size = 1, prob = prob_Y)
list(Y = Y, pY1 = pY(1,W), pY0 = pY(0,W))
}
simulate_from_model <- function(n = 100){
#Simulate covariates
W <- sim_cov(n)
#simulate A
A <- sim_A(W)
#Simulate Y
Y_lst <- sim_Y(A,W)
Y <- Y_lst$Y
ATE <- mean(Y_lst$pY1 - Y_lst$pY0)
# Bind W, A, Y together to tibble, such that the columns in W are named w1, w2, ..., w10
out_frame <- cbind(W, A, Y) %>% as_tibble()
list(
ATE = ATE,
out_frame = out_frame
)
}
simulate_from_model()
simulate_from_model()
simulate_from_model()
source("Estimator.R")
source("LearnerTypes.R")
source("simulate.R")
#Initialise estimator Class with using GLMnet as propensity learner and GLMnet as mean learner
mean_lrn <- GLM$new(Y ~ w1 + w2 + A)
estimator <- Estimator$new(prp_lrn = GLMNet$new(A ~ w2), mean_lrn = mean_lrn)
#Repeat experiment 1000 times, each time simulating 500 data points
theta <- numeric(1000)
means <- numeric(1000)
for(i in 1:1000){
sim_data <- simulate_from_model(n = 500)
theta[i] <- estimator$fit(sim_data$out_frame, one_step = FALSE, cross_fit = FALSE)
ATE[i] <- sim_data$ATE
print(i)
}
simulate_from_model(n = 500)
simulate_from_model(n = 500)$ATE
source("Estimator.R")
source("LearnerTypes.R")
source("simulate.R")
#Initialise estimator Class with using GLMnet as propensity learner and GLMnet as mean learner
mean_lrn <- GLM$new(Y ~ w1 + w2 + A)
estimator <- Estimator$new(prp_lrn = GLMNet$new(A ~ w2), mean_lrn = mean_lrn)
#Repeat experiment 1000 times, each time simulating 500 data points
theta <- numeric(1000)
means <- numeric(1000)
for(i in 1:1000){
sim_data <- simulate_from_model(n = 500)
theta[i] <- estimator$fit(sim_data$out_frame, one_step = FALSE, cross_fit = FALSE)
ATE[i] <- sim_data$ATE
print(i)
}
sim_data <- simulate_from_model(n = 500)
theta[i] <- estimator$fit(sim_data$out_frame, one_step = FALSE, cross_fit = FALSE)
theta
sim_data$ATE
ATE[i] <- sim_data$ATE
source("Estimator.R")
source("LearnerTypes.R")
source("simulate.R")
#Initialise estimator Class with using GLMnet as propensity learner and GLMnet as mean learner
mean_lrn <- GLM$new(Y ~ w1 + w2 + A)
estimator <- Estimator$new(prp_lrn = GLMNet$new(A ~ w2), mean_lrn = mean_lrn)
#Repeat experiment 1000 times, each time simulating 500 data points
theta <- numeric(1000)
ATE <- numeric(1000)
for(i in 1:1000){
sim_data <- simulate_from_model(n = 500)
theta[i] <- estimator$fit(sim_data$out_frame, one_step = FALSE, cross_fit = FALSE)
ATE[i] <- sim_data$ATE
print(i)
}
library(ggplot2)
#make dataframe of theta values and plot histogram with a vertical line at
df <- tibble(theta = theta)
estimator$mean_lrn$predict(sim_data$out_frame)
mean(sim_data$true_cond_mean$cond_Y_true-estimator$mean_lrn$predict(sim_data$out_frame))
ggplot(df, aes(x = theta)) + geom_histogram(bins = 10) + geom_vline(xintercept = mean(means), color = "red") + theme_minimal()
ATE
mean(ATE)
ggplot(df, aes(x = theta)) + geom_histogram(bins = 10) + geom_vline(xintercept = mean(ATE), color = "red") + theme_minimal()
source("Estimator.R")
source("LearnerTypes.R")
source("simulate.R")
#Initialise estimator Class with using GLMnet as propensity learner and GLMnet as mean learner
mean_lrn <- GLM$new(Y ~ w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8 + w9 + w10 + A)
estimator <- Estimator$new(prp_lrn = GLMNet$new(A ~ w2), mean_lrn = mean_lrn)
#Repeat experiment 1000 times, each time simulating 500 data points
theta <- numeric(1000)
ATE <- numeric(1000)
for(i in 1:1000){
sim_data <- simulate_from_model(n = 500)
theta[i] <- estimator$fit(sim_data$out_frame, one_step = FALSE, cross_fit = FALSE)
ATE[i] <- sim_data$ATE
print(i)
}
library(ggplot2)
#make dataframe of theta values and plot histogram with a vertical line at
df <- tibble(theta = theta)
estimator$mean_lrn$predict(sim_data$out_frame)
mean(sim_data$true_cond_mean$cond_Y_true-estimator$mean_lrn$predict(sim_data$out_frame))
ggplot(df, aes(x = theta)) + geom_histogram(bins = 10) + geom_vline(xintercept = mean(ATE), color = "red") + theme_minimal()
source("Estimator.R")
source("LearnerTypes.R")
source("simulate.R")
#Initialise estimator Class with using GLMnet as propensity learner and GLMnet as mean learner
mean_lrn <- GLM$new(Y ~ w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8 + w9 + w10 + A)
estimator <- Estimator$new(prp_lrn = GLMNet$new(A ~ w2), mean_lrn = mean_lrn)
#Repeat experiment 1000 times, each time simulating 500 data points
theta <- numeric(1000)
ATE <- numeric(1000)
for(i in 1:1000){
sim_data <- simulate_from_model(n = 500)
theta[i] <- estimator$fit(sim_data$out_frame, one_step = FALSE, cross_fit = FALSE)
ATE[i] <- sim_data$ATE
print(i)
}
library(ggplot2)
#make dataframe of theta values and plot histogram with a vertical line at
df <- tibble(theta = theta)
estimator$mean_lrn$predict(sim_data$out_frame)
mean(sim_data$true_cond_mean$cond_Y_true-estimator$mean_lrn$predict(sim_data$out_frame))
ggplot(df, aes(x = theta)) + geom_histogram(bins = 10) + geom_vline(xintercept = mean(ATE), color = "red") + theme_minimal()
ggplot(df, aes(x = sqrt(50)*theta)) + geom_histogram(bins = 10) + geom_vline(xintercept = mean(ATE), color = "red") + theme_minimal()
ggplot(df, aes(x = sqrt(500)*theta)) + geom_histogram(bins = 10) + geom_vline(xintercept = mean(ATE), color = "red") + theme_minimal()
Ate
ATE
mean(ATE)
theta
mean(theta)
source("Estimator.R")
source("LearnerTypes.R")
source("simulate.R")
#Initialise estimator Class with using GLMnet as propensity learner and GLMnet as mean learner
mean_lrn <- GLM$new(Y ~ w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8 + w9 + w10 + A)
estimator <- Estimator$new(prp_lrn = GLMNet$new(A ~ w2), mean_lrn = mean_lrn)
#Repeat experiment 1000 times, each time simulating 500 data points
theta <- numeric(1000)
ATE <- numeric(1000)
for(i in 1:1000){
sim_data <- simulate_from_model(n = 500)
theta[i] <- estimator$fit(sim_data$out_frame, one_step = FALSE, cross_fit = FALSE)
ATE[i] <- sim_data$ATE
print(i)
}
library(ggplot2)
#make dataframe of theta values and plot histogram with a vertical line at
df <- tibble(theta = theta)
estimator$mean_lrn$predict(sim_data$out_frame)
mean(sim_data$true_cond_mean$cond_Y_true-estimator$mean_lrn$predict(sim_data$out_frame))
ggplot(df, aes(x = sqrt(500)*theta)) + geom_histogram(bins = 10) + geom_vline(xintercept = mean(ATE), color = "red") + theme_minimal()
source("Estimator.R")
source("LearnerTypes.R")
source("simulate.R")
#Initialise estimator Class with using GLMnet as propensity learner and GLMnet as mean learner
mean_lrn <- GLM$new(Y ~ w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8 + w9 + w10 + A)
estimator <- Estimator$new(prp_lrn = GLMNet$new(A ~ w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8 + w9 + w10 ), mean_lrn = mean_lrn)
#Repeat experiment 1000 times, each time simulating 500 data points
theta <- numeric(1000)
ATE <- numeric(1000)
for(i in 1:1000){
sim_data <- simulate_from_model(n = 500)
theta[i] <- estimator$fit(sim_data$out_frame, one_step = TRUE, cross_fit = FALSE)
ATE[i] <- sim_data$ATE
print(i)
}
library(ggplot2)
#make dataframe of theta values and plot histogram with a vertical line at
df <- tibble(theta = theta)
estimator$mean_lrn$predict(sim_data$out_frame)
mean(sim_data$true_cond_mean$cond_Y_true-estimator$mean_lrn$predict(sim_data$out_frame))
ggplot(df, aes(x = sqrt(500)*theta)) + geom_histogram(bins = 10) + geom_vline(xintercept = mean(ATE), color = "red") + theme_minimal()
source("Estimator.R")
source("LearnerTypes.R")
source("simulate.R")
#Initialise estimator Class with using GLMnet as propensity learner and GLMnet as mean learner
mean_lrn <- GLM$new(Y ~ w1 + w2 + w3 + w9 + w10 + A)
estimator <- Estimator$new(prp_lrn = GLMNet$new(A ~ w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8 + w9 + w10 ), mean_lrn = mean_lrn)
#Repeat experiment 1000 times, each time simulating 500 data points
theta <- numeric(1000)
ATE <- numeric(1000)
for(i in 1:1000){
sim_data <- simulate_from_model(n = 500)
theta[i] <- estimator$fit(sim_data$out_frame, one_step = FALSE, cross_fit = FALSE)
ATE[i] <- sim_data$ATE
print(i)
}
library(ggplot2)
#make dataframe of theta values and plot histogram with a vertical line at
df <- tibble(theta = theta)
estimator$mean_lrn$predict(sim_data$out_frame)
mean(sim_data$true_cond_mean$cond_Y_true-estimator$mean_lrn$predict(sim_data$out_frame))
ggplot(df, aes(x = sqrt(500)*theta)) + geom_histogram(bins = 10) + geom_vline(xintercept = mean(ATE), color = "red") + theme_minimal()
source("Estimator.R")
source("LearnerTypes.R")
source("simulate.R")
#Initialise estimator Class with using GLMnet as propensity learner and GLMnet as mean learner
mean_lrn <- GLM$new(Y ~ w9 + w10 + A)
estimator <- Estimator$new(prp_lrn = GLMNet$new(A ~ w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8 + w9 + w10 ), mean_lrn = mean_lrn)
#Repeat experiment 1000 times, each time simulating 500 data points
theta <- numeric(1000)
ATE <- numeric(1000)
for(i in 1:1000){
sim_data <- simulate_from_model(n = 500)
theta[i] <- estimator$fit(sim_data$out_frame, one_step = FALSE, cross_fit = FALSE)
ATE[i] <- sim_data$ATE
print(i)
}
library(ggplot2)
#make dataframe of theta values and plot histogram with a vertical line at
df <- tibble(theta = theta)
estimator$mean_lrn$predict(sim_data$out_frame)
mean(sim_data$true_cond_mean$cond_Y_true-estimator$mean_lrn$predict(sim_data$out_frame))
ggplot(df, aes(x = sqrt(500)*theta)) + geom_histogram(bins = 10) + geom_vline(xintercept = mean(ATE), color = "red") + theme_minimal()
source("Estimator.R")
source("LearnerTypes.R")
source("simulate.R")
#Initialise estimator Class with using GLMnet as propensity learner and GLMnet as mean learner
mean_lrn <- GLM$new(Y ~ w9 + w10 + A)
estimator <- Estimator$new(prp_lrn = GLM$new(A ~ w1 + w2 + w3), mean_lrn = mean_lrn)
#Repeat experiment 1000 times, each time simulating 500 data points
theta <- numeric(1000)
ATE <- numeric(1000)
for(i in 1:1000){
sim_data <- simulate_from_model(n = 500)
theta[i] <- estimator$fit(sim_data$out_frame, one_step = TRUE, cross_fit = FALSE)
ATE[i] <- sim_data$ATE
print(i)
}
library(ggplot2)
#make dataframe of theta values and plot histogram with a vertical line at
df <- tibble(theta = theta)
estimator$mean_lrn$predict(sim_data$out_frame)
mean(sim_data$true_cond_mean$cond_Y_true-estimator$mean_lrn$predict(sim_data$out_frame))
ggplot(df, aes(x = sqrt(500)*theta)) + geom_histogram(bins = 10) + geom_vline(xintercept = mean(ATE), color = "red") + theme_minimal()
ggplot(df, aes(x = sqrt(500)*theta)) + geom_histogram(bins = 200) + geom_vline(xintercept = mean(ATE), color = "red") + theme_minimal()
ggplot(df, aes(x = sqrt(500)*theta)) + geom_histogram(bins = 100) + geom_vline(xintercept = mean(ATE), color = "red") + theme_minimal()
ATE
mean(ATE)
source("Estimator.R")
source("LearnerTypes.R")
source("simulate.R")
#Initialise estimator Class with using GLMnet as propensity learner and GLMnet as mean learner
mean_lrn <- GLM$new(Y ~ w9 + w10 + A)
estimator <- Estimator$new(prp_lrn = GLM$new(A ~ w1 + w2 + w3), mean_lrn = mean_lrn)
#Repeat experiment 1000 times, each time simulating 500 data points
theta <- numeric(1000)
ATE <- numeric(1000)
for(i in 1:1000){
sim_data <- simulate_from_model(n = 500)
theta[i] <- estimator$fit(sim_data$out_frame, one_step = TRUE, cross_fit = FALSE)
ATE[i] <- sim_data$ATE
print(i)
}
library(ggplot2)
#make dataframe of theta values and plot histogram with a vertical line at
df <- tibble(theta = theta)
estimator$mean_lrn$predict(sim_data$out_frame)
mean(sim_data$true_cond_mean$cond_Y_true-estimator$mean_lrn$predict(sim_data$out_frame))
ggplot(df, aes(x = sqrt(500)*theta)) + geom_histogram(bins = 100) + geom_vline(xintercept = mean(ATE), color = "red") + theme_minimal()
Ate
ATE
mean(ATE)
ggplot(df, aes(x = sqrt(500)*(theta-mean(ATE)))) + geom_histogram(bins = 100) + geom_vline(xintercept = 0, color = "red") + theme_minimal()
source("Estimator.R")
source("LearnerTypes.R")
source("simulate.R")
#Initialise estimator Class with using GLMnet as propensity learner and GLMnet as mean learner
mean_lrn <- GLM$new(Y ~ w9 + w10 + A)
estimator <- Estimator$new(prp_lrn = GLM$new(A ~ w1 + w2 + w3), mean_lrn = mean_lrn)
#Repeat experiment 1000 times, each time simulating 500 data points
theta <- numeric(1000)
ATE <- numeric(1000)
for(i in 1:1000){
sim_data <- simulate_from_model(n = 500)
theta[i] <- estimator$fit(sim_data$out_frame, one_step = FALSE, cross_fit = FALSE)
ATE[i] <- sim_data$ATE
print(i)
}
library(ggplot2)
#make dataframe of theta values and plot histogram with a vertical line at
df <- tibble(theta = theta)
estimator$mean_lrn$predict(sim_data$out_frame)
mean(sim_data$true_cond_mean$cond_Y_true-estimator$mean_lrn$predict(sim_data$out_frame))
ggplot(df, aes(x = sqrt(500)*(theta-mean(ATE)))) + geom_histogram(bins = 100) + geom_vline(xintercept = 0, color = "red") + theme_minimal()
source("Estimator.R")
source("LearnerTypes.R")
source("simulate.R")
#Initialise estimator Class with using GLMnet as propensity learner and GLMnet as mean learner
mean_lrn <- GLM$new(Y ~ w9 + w10 + A)
estimator <- Estimator$new(prp_lrn = GLM$new(A ~ w1 + w2 + w3), mean_lrn = mean_lrn)
#Repeat experiment 1000 times, each time simulating 500 data points
theta <- numeric(1000)
ATE <- numeric(1000)
for(i in 1:1000){
sim_data <- simulate_from_model(n = 500)
theta[i] <- estimator$fit(sim_data$out_frame, one_step = TRUE, cross_fit = FALSE)
ATE[i] <- sim_data$ATE
print(i)
}
library(ggplot2)
#make dataframe of theta values and plot histogram with a vertical line at
df <- tibble(theta = theta)
estimator$mean_lrn$predict(sim_data$out_frame)
mean(sim_data$true_cond_mean$cond_Y_true-estimator$mean_lrn$predict(sim_data$out_frame))
ggplot(df, aes(x = sqrt(500)*(theta-mean(ATE)))) + geom_histogram(bins = 100) + geom_vline(xintercept = 0, color = "red") + theme_minimal()
ggplot(df, aes(x = sqrt(500)*(theta-mean(ATE)))) + geom_histogram(bins = 20) + geom_vline(xintercept = 0, color = "red") + theme_minimal()
ggplot(df, aes(x = sqrt(500)*(theta-mean(ATE)))) + geom_histogram(bins = 50) + geom_vline(xintercept = 0, color = "red") + theme_minimal()
source("Estimator.R")
source("LearnerTypes.R")
source("simulate.R")
#Initialise estimator Class with using GLMnet as propensity learner and GLMnet as mean learner
mean_lrn <- GLM$new(Y ~ w9 + w10 + A)
estimator <- Estimator$new(prp_lrn = GLM$new(A ~ w1 + w2 + w3), mean_lrn = mean_lrn)
#Repeat experiment 1000 times, each time simulating 500 data points
theta <- numeric(1000)
ATE <- numeric(1000)
for(i in 1:1000){
sim_data <- simulate_from_model(n = 500)
theta[i] <- estimator$fit(sim_data$out_frame, one_step = FALSE, cross_fit = FALSE)
ATE[i] <- sim_data$ATE
print(i)
}
library(ggplot2)
#make dataframe of theta values and plot histogram with a vertical line at
df <- tibble(theta = theta)
estimator$mean_lrn$predict(sim_data$out_frame)
mean(sim_data$true_cond_mean$cond_Y_true-estimator$mean_lrn$predict(sim_data$out_frame))
ggplot(df, aes(x = sqrt(500)*(theta-mean(ATE)))) + geom_histogram(bins = 50) + geom_vline(xintercept = 0, color = "red") + theme_minimal()
mean(ATE)
